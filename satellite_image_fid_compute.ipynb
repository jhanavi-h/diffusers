{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wioh7QjPx0ac",
        "outputId": "bc2d8b1f-dbc8-4435-bb09-ff57c21a912d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/satellite_images/diffusers\n",
            "Processing /content/gdrive/MyDrive/satellite_images/diffusers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers==0.30.0.dev0) (8.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.30.0.dev0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.30.0.dev0) (0.24.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers==0.30.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.30.0.dev0) (2024.9.11)\n",
            "Collecting requests==2.31.0 (from diffusers==0.30.0.dev0)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.30.0.dev0) (0.4.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers==0.30.0.dev0) (10.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->diffusers==0.30.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->diffusers==0.30.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->diffusers==0.30.0.dev0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->diffusers==0.30.0.dev0) (2024.8.30)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.30.0.dev0) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.30.0.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.30.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.30.0.dev0) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.30.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers==0.30.0.dev0) (3.20.2)\n",
            "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: diffusers\n",
            "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.30.0.dev0-py3-none-any.whl size=2209968 sha256=54794491fb51435a487b68e9857c2fb9f872c435a008c6dd99487be1c04170fc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gyzpdec1/wheels/fc/84/1b/ca6d366a271922ef0c65f5bbbfb0c2869095a047aea4c00a4b\n",
            "Successfully built diffusers\n",
            "Installing collected packages: requests, diffusers\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: diffusers\n",
            "    Found existing installation: diffusers 0.30.3\n",
            "    Uninstalling diffusers-0.30.3:\n",
            "      Successfully uninstalled diffusers-0.30.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed diffusers-0.30.0.dev0 requests-2.31.0\n"
          ]
        }
      ],
      "source": [
        "# Mount Google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ln -s /content/gdrive/MyDrive /mydrive\n",
        "%cd /mydrive/satellite_images/diffusers/\n",
        "\n",
        "# Install python packages and dependencies\n",
        "# Ignore pip error for now\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTGHTKoNynGc",
        "outputId": "c409caad-f863-4769-aede-0f52a6b204fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/satellite_images/diffusers/examples/consistency_distillation\n",
            "Requirement already satisfied: accelerate>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.34.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.20.0+cu121)\n",
            "Collecting datasets==2.16.0 (from -r requirements.txt (line 3))\n",
            "  Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.18.5)\n",
            "Collecting bitsandbytes (from -r requirements.txt (line 5))\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting xformers (from -r requirements.txt (line 6))\n",
            "  Downloading xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (4.44.2)\n",
            "Collecting ftfy (from -r requirements.txt (line 8))\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.17.0)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (3.1.4)\n",
            "Collecting webdataset (from -r requirements.txt (line 11))\n",
            "  Downloading webdataset-0.2.100-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.0->-r requirements.txt (line 3)) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.0->-r requirements.txt (line 3)) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.0->-r requirements.txt (line 3)) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.0->-r requirements.txt (line 3)) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.16.0->-r requirements.txt (line 3))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.0->-r requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.0->-r requirements.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.0->-r requirements.txt (line 3)) (4.66.5)\n",
            "Collecting xxhash (from datasets==2.16.0->-r requirements.txt (line 3))\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets==2.16.0->-r requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.0->-r requirements.txt (line 3))\n",
            "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.0->-r requirements.txt (line 3)) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.0->-r requirements.txt (line 3)) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.0->-r requirements.txt (line 3)) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.0->-r requirements.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.16.0->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.16.0->-r requirements.txt (line 1)) (2.5.0+cu121)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.16.0->-r requirements.txt (line 1)) (0.4.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 2)) (10.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 4)) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 4)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 4)) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 4)) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 4)) (3.20.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 4)) (2.17.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 4)) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 4)) (75.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 7)) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 7)) (0.19.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->-r requirements.txt (line 8)) (0.2.13)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 9)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 9)) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 9)) (3.7)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 9)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 9)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 9)) (3.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2->-r requirements.txt (line 10)) (3.0.2)\n",
            "Collecting braceexpand (from webdataset->-r requirements.txt (line 11))\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.0->-r requirements.txt (line 3)) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.0->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.0->-r requirements.txt (line 3)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.0->-r requirements.txt (line 3)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.0->-r requirements.txt (line 3)) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.0->-r requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.0->-r requirements.txt (line 3)) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 4)) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.16.0->-r requirements.txt (line 3)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.16.0->-r requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.16.0->-r requirements.txt (line 3)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.16.0->-r requirements.txt (line 3)) (2024.8.30)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.16.0->-r requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.16.0->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.16.0->-r requirements.txt (line 3)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.16.0->-r requirements.txt (line 3)) (2024.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 4)) (5.0.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==2.16.0->-r requirements.txt (line 3)) (0.2.0)\n",
            "Downloading datasets-2.16.0-py3-none-any.whl (507 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl (16.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdataset-0.2.100-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: braceexpand, xxhash, webdataset, ftfy, fsspec, dill, multiprocess, xformers, bitsandbytes, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.44.1 braceexpand-0.1.7 datasets-2.16.0 dill-0.3.7 fsspec-2023.10.0 ftfy-6.3.1 multiprocess-0.70.15 webdataset-0.2.100 xformers-0.0.28.post2 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Install more package dependencies\n",
        "%cd /content/gdrive/MyDrive/satellite_images/diffusers/examples/consistency_distillation\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "!pip install torch-fidelity\n",
        "!pip install torchmetrics\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "_ = torch.manual_seed(123)\n",
        "\n",
        "fid = FrechetInceptionDistance(feature=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFj5Y0EmBiro",
        "outputId": "06aed570-7c11-4e66-8da9-358e10588dff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-fidelity\n",
            "  Downloading torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-fidelity) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from torch-fidelity) (10.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-fidelity) (1.13.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torch-fidelity) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from torch-fidelity) (0.20.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-fidelity) (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torch-fidelity) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torch-fidelity) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torch-fidelity) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torch-fidelity) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torch-fidelity) (2023.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->torch-fidelity) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->torch-fidelity) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torch-fidelity) (3.0.2)\n",
            "Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: torch-fidelity\n",
            "Successfully installed torch-fidelity-0.3.0\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.5.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0,>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.0+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.5.1-py3-none-any.whl (890 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.6/890.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.8 torchmetrics-1.5.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n",
            "100%|██████████| 91.2M/91.2M [00:00<00:00, 442MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FhII7-z_Xe9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5vK0iytnKfkA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C60Ubak3cK4R"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjbSwHzuoWoC",
        "outputId": "e681fe52-c6a2-461e-d2b3-888cf91da242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:A matching Triton is not available, some optimizations will not be enabled\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xformers/__init__.py\", line 57, in _is_triton_available\n",
            "    import triton  # noqa\n",
            "ModuleNotFoundError: No module named 'triton'\n",
            "/usr/local/lib/python3.10/dist-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
            "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
          ]
        }
      ],
      "source": [
        "#%cd /mydrive/satellite_images/diffusers/\n",
        "#from utils import parse_args, train_student\n",
        "\n",
        "import sys, importlib\n",
        "import torch\n",
        "sys.path.append('/mydrive/satellite_images/diffusers/')\n",
        "utils = importlib.import_module('utils')\n",
        "from utils import parse_args, train_student\n",
        "\n",
        "args = parse_args()\n",
        "args.push_to_hub = False\n",
        "args.gradient_checkpointing = False\n",
        "args.tracker_project_name = \"super_resolution_distillation\"\n",
        "\n",
        "# hyper parameter for distillation\n",
        "#args.loss_type = \"l2\"\n",
        "#args.num_ddim_timesteps = 40\n",
        "#timestep_scaling_factor = 20\n",
        "#train_student_model(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tX28yBzWWTTk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQpAXnv9YT24",
        "outputId": "3a9b19bc-9b15-4d6a-8ead-4df942aaffbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/satellite_images/diffusers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:A matching Triton is not available, some optimizations will not be enabled\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xformers/__init__.py\", line 57, in _is_triton_available\n",
            "    import triton  # noqa\n",
            "ModuleNotFoundError: No module named 'triton'\n",
            "/usr/local/lib/python3.10/dist-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
            "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n",
            "<ipython-input-5-3168dbb50938>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pipeline.vqvae.load_state_dict(torch.load(VQVAE_PATH))\n",
            "<ipython-input-5-3168dbb50938>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pipeline.unet.load_state_dict(torch.load(UNET_PATH))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UNet2DModel(\n",
              "  (conv_in): Conv2d(6, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (time_proj): Timesteps()\n",
              "  (time_embedding): TimestepEmbedding(\n",
              "    (linear_1): Linear(in_features=160, out_features=640, bias=True)\n",
              "    (act): SiLU()\n",
              "    (linear_2): Linear(in_features=640, out_features=640, bias=True)\n",
              "  )\n",
              "  (down_blocks): ModuleList(\n",
              "    (0): DownBlock2D(\n",
              "      (resnets): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 160, eps=1e-05, affine=True)\n",
              "          (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): Linear(in_features=640, out_features=160, bias=True)\n",
              "          (norm2): GroupNorm(32, 160, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "        )\n",
              "      )\n",
              "      (downsamplers): ModuleList(\n",
              "        (0): Downsample2D(\n",
              "          (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): DownBlock2D(\n",
              "      (resnets): ModuleList(\n",
              "        (0): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 160, eps=1e-05, affine=True)\n",
              "          (conv1): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): Linear(in_features=640, out_features=320, bias=True)\n",
              "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): Linear(in_features=640, out_features=320, bias=True)\n",
              "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "        )\n",
              "      )\n",
              "      (downsamplers): ModuleList(\n",
              "        (0): Downsample2D(\n",
              "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): DownBlock2D(\n",
              "      (resnets): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): Linear(in_features=640, out_features=320, bias=True)\n",
              "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "        )\n",
              "      )\n",
              "      (downsamplers): ModuleList(\n",
              "        (0): Downsample2D(\n",
              "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): AttnDownBlock2D(\n",
              "      (attentions): ModuleList(\n",
              "        (0-1): 2 x Attention(\n",
              "          (group_norm): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "          (to_q): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (to_k): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (to_v): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (to_out): ModuleList(\n",
              "            (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "            (1): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "          (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "          (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (up_blocks): ModuleList(\n",
              "    (0): AttnUpBlock2D(\n",
              "      (attentions): ModuleList(\n",
              "        (0-2): 3 x Attention(\n",
              "          (group_norm): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "          (to_q): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (to_k): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (to_v): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (to_out): ModuleList(\n",
              "            (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "            (1): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
              "          (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
              "          (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (upsamplers): ModuleList(\n",
              "        (0): Upsample2D(\n",
              "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): UpBlock2D(\n",
              "      (resnets): ModuleList(\n",
              "        (0): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
              "          (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): Linear(in_features=640, out_features=320, bias=True)\n",
              "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1-2): 2 x ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "          (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): Linear(in_features=640, out_features=320, bias=True)\n",
              "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (upsamplers): ModuleList(\n",
              "        (0): Upsample2D(\n",
              "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): UpBlock2D(\n",
              "      (resnets): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "          (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): Linear(in_features=640, out_features=320, bias=True)\n",
              "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 480, eps=1e-05, affine=True)\n",
              "          (conv1): Conv2d(480, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): Linear(in_features=640, out_features=320, bias=True)\n",
              "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): Conv2d(480, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (upsamplers): ModuleList(\n",
              "        (0): Upsample2D(\n",
              "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): UpBlock2D(\n",
              "      (resnets): ModuleList(\n",
              "        (0): ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 480, eps=1e-05, affine=True)\n",
              "          (conv1): Conv2d(480, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): Linear(in_features=640, out_features=160, bias=True)\n",
              "          (norm2): GroupNorm(32, 160, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1-2): 2 x ResnetBlock2D(\n",
              "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
              "          (conv1): Conv2d(320, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (time_emb_proj): Linear(in_features=640, out_features=160, bias=True)\n",
              "          (norm2): GroupNorm(32, 160, eps=1e-05, affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "          (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv_shortcut): Conv2d(320, 160, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (mid_block): UNetMidBlock2D(\n",
              "    (attentions): ModuleList(\n",
              "      (0): Attention(\n",
              "        (group_norm): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "        (to_q): Linear(in_features=640, out_features=640, bias=True)\n",
              "        (to_k): Linear(in_features=640, out_features=640, bias=True)\n",
              "        (to_v): Linear(in_features=640, out_features=640, bias=True)\n",
              "        (to_out): ModuleList(\n",
              "          (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (1): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (resnets): ModuleList(\n",
              "      (0-1): 2 x ResnetBlock2D(\n",
              "        (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (time_emb_proj): Linear(in_features=640, out_features=640, bias=True)\n",
              "        (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (nonlinearity): SiLU()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (conv_norm_out): GroupNorm(32, 160, eps=1e-05, affine=True)\n",
              "  (conv_act): SiLU()\n",
              "  (conv_out): Conv2d(160, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#Load LDM consistency models\n",
        "\n",
        "%cd /mydrive/satellite_images/diffusers/\n",
        "#pipeline_ldm_consistency_sr = importlib.import_module('pipeline_ldm_consistency_sr')\n",
        "from pipeline_ldm_consistency_sr import LDMConsistencySRPipeline\n",
        "pipeline = LDMConsistencySRPipeline() #.from_pretrained(\"CompVis/ldm-super-resolution-4x-openimages\")\n",
        "UNET_PATH = \"/content/gdrive/MyDrive/satellite_images/diffusers/models/con_unet_model_080824.pt\"\n",
        "VQVAE_PATH = \"/content/gdrive/MyDrive/satellite_images/diffusers/models/con_vqvae_model_080824.pt\"\n",
        "\n",
        "# Use CUDA/GPU if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "pipeline.to(device)\n",
        "pipeline.vqvae.load_state_dict(torch.load(VQVAE_PATH))\n",
        "pipeline.unet.load_state_dict(torch.load(UNET_PATH))\n",
        "pipeline.vqvae.to(device)\n",
        "pipeline.unet.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "IMAGE_PATH = \"/content/gdrive/MyDrive/satellite_images/dataset/test/images/\"\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, num_images=10000):\n",
        "        self.root_dir = root_dir\n",
        "        #self.transform = transform\n",
        "        self.images = []\n",
        "\n",
        "        for count, image_path in enumerate(os.listdir(root_dir)):\n",
        "            #image = Image.open(os.path.join(root_dir, image_path))\n",
        "            self.images.append(image_path)\n",
        "            if (count >= (num_images-1)):\n",
        "               break\n",
        "\n",
        "    def transform1(self, image):\n",
        "            # image to tensor and normalize\n",
        "            (w,h) = image.size\n",
        "            image = TF.to_tensor(image)\n",
        "            lr_image = TF.resize(image, (h//4, w//4))\n",
        "            image = TF.normalize(image, [0.5], [0.5])\n",
        "            lr_image = TF.normalize(lr_image, [0.5], [0.5])\n",
        "            return image, lr_image\n",
        "\n",
        "\n",
        "    # Image transforms for dataset\n",
        "    def transform(self, image, resolution=512):\n",
        "\n",
        "       # crop if size greater than 512 x 512\n",
        "       def crop_normalize(image):\n",
        "       # get crop coordinates and crop image\n",
        "         (w,h) = image.size\n",
        "         resolution = 512\n",
        "\n",
        "         if (w < resolution or h < resolution):\n",
        "           if (w<=h):\n",
        "             image = TF.crop(image,0,0,w,w)\n",
        "           else:\n",
        "             image = TF.crop(image,0,0,h,h)\n",
        "\n",
        "           image = TF.resize(image, (resolution, resolution))\n",
        "         else:\n",
        "           c_top, c_left, _, _ = transforms.RandomCrop.get_params(image, output_size=(resolution, resolution))\n",
        "           image = TF.crop(image, c_top, c_left, resolution, resolution)\n",
        "\n",
        "         image = TF.to_tensor(image)\n",
        "         image = TF.normalize(image, [0.5], [0.5])\n",
        "         return image\n",
        "\n",
        "       image = crop_normalize(image)\n",
        "       lr_image = TF.resize(image, (resolution//4, resolution//4))\n",
        "       return image, lr_image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = self.images[index]\n",
        "        image = Image.open(os.path.join(self.root_dir, image_path))\n",
        "        #image.filter(ImageFilter.GaussianBlur(radius=2))\n",
        "        image, lr_image = self.transform(image)\n",
        "        return ({\"hr_images\":image, \"lr_images\":lr_image})\n",
        "\n",
        "# Create a dataloader\n",
        "#train_dataloader = DataLoader(ImageDataset(root_dir=IMAGE_PATH), batch_size=8, shuffle=True)\n",
        "dataset = ImageDataset(root_dir=IMAGE_PATH, num_images=2000)\n",
        "#train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "eval_dataloader = DataLoader(dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "whyUWJ_R5-BA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VZ7N5nAx1inD"
      },
      "outputs": [],
      "source": [
        "# Inference with 4 steps\n",
        "# DDPM would need 100 steps\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import InterpolationMode\n",
        "# Create a resize transform\n",
        "resize_transform = T.Resize((512, 512),interpolation=InterpolationMode.NEAREST)\n",
        "\n",
        "fid_sr = FrechetInceptionDistance(feature=64)\n",
        "fid_lr = FrechetInceptionDistance(feature=64)\n",
        "\n",
        "num_inference_steps = 4\n",
        "#PSNR_values = []\n",
        "sr_mse_values = []\n",
        "lr_mse_values = []\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "pipeline.vqvae.to(device)\n",
        "\n",
        "pipeline.unet.to(device)\n",
        "pipeline.to(device)\n",
        "\n",
        "def update_FID(fid, real_image, gen_image):\n",
        "  real_img = ((real_image/2.0 + 0.5)*255).to(torch.uint8)\n",
        "  gen_img = ((gen_image/2.0 + 0.5)*255).to(torch.uint8)\n",
        "  fid.update(real_img, real=True)\n",
        "  fid.update(gen_img, real=False)\n",
        "  return\n",
        "\n",
        "def compute_MSE(org_image, decode_image):\n",
        "    shape = org_image.size()\n",
        "\n",
        "    # Calculate MSE loss\n",
        "    loss = nn.MSELoss(reduction='none')\n",
        "    loss_result = torch.sum(loss(org_image,decode_image))\n",
        "    return(loss_result/(shape[0]*shape[1]*shape[2]*shape[3]))\n",
        "\n",
        "for step, batch in enumerate(eval_dataloader):\n",
        "    # 1. Load and process the image and text conditioning\n",
        "\n",
        "\n",
        "    hr_images = batch[\"hr_images\"]\n",
        "    lr_images = batch[\"lr_images\"]\n",
        "    sr_images = []\n",
        "    for image_index in range(lr_images.shape[0]):\n",
        "       (sr_image, dummy) = pipeline(lr_images[image_index:image_index+1],\n",
        "                        num_inference_steps=num_inference_steps,\n",
        "                        return_intermediate_images = False)\n",
        "       sr_images.append(sr_image[0])\n",
        "\n",
        "    sr_images = torch.stack(sr_images).to('cpu')\n",
        "    lr_images = resize_transform(lr_images)\n",
        "\n",
        "    #hr_images = hr_images.permute(0, 2, 3, 1)\n",
        "    lr_images = resize_transform(lr_images)#.permute(0, 2, 3, 1)\n",
        "    update_FID(fid_sr, hr_images, sr_images)\n",
        "    update_FID(fid_lr, hr_images, lr_images)\n",
        "\n",
        "    sr_mse_values.append(compute_MSE(hr_images.permute(0, 2, 3, 1),\n",
        "                                    sr_images.permute(0, 2, 3, 1)))\n",
        "    lr_mse_values.append(compute_MSE(hr_images.permute(0, 2, 3, 1),\n",
        "                                     lr_images.permute(0, 2, 3, 1)))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"FID Score of Low resolution pictures ia {fid_lr.compute()}\")\n",
        "print(f\"FID Score of Super resolution pictures ia {fid_sr.compute()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQAnpClVSO79",
        "outputId": "76dc7db8-6354-4619-b3b9-68186aea6a6b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FID Score of Low resolution pictures ia 10.799049377441406\n",
            "FID Score of Super resolution pictures ia 1.9115413427352905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v5lbr7GDF0oU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HoGCzPjCAh9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TBZnioX6mJ_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vY8p7VaF1iH1"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}